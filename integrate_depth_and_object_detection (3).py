# -*- coding: utf-8 -*-
"""Integrate Depth and Object Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KgdeL2K6SGHm7NLzYb1TomOlQbb5aqNj

# Installing DepthAnything for Depth Estimation
"""

# Commented out IPython magic to ensure Python compatibility.
from pathlib import Path


repo_dir = Path("Depth-Anything-V2")

if not repo_dir.exists():
    !git clone https://huggingface.co/spaces/depth-anything/Depth-Anything-V2
# %cd Depth-Anything-V2

# Commented out IPython magic to ensure Python compatibility.
import platform

# %pip install -q "openvino>=2024.2.0" "datasets>=2.14.6" "nncf>=2.11.0" "tqdm" "matplotlib>=3.4"
# %pip install -q "typing-extensions>=4.9.0" eval-type-backport "gradio>=4.19"
# %pip install -q -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu

if platform.python_version_tuple()[1] in ["8", "9"]:
#     %pip install -q "gradio-imageslider<=0.0.17" "typing-extensions>=4.9.0"

attention_file_path = Path("./depth_anything_v2/dinov2_layers/attention.py")
orig_attention_path = attention_file_path.parent / ("orig_" + attention_file_path.name)

if not orig_attention_path.exists():
    attention_file_path.rename(orig_attention_path)

    with orig_attention_path.open("r") as f:
        data = f.read()
        data = data.replace("XFORMERS_AVAILABLE = True", "XFORMERS_AVAILABLE = False")
        with attention_file_path.open("w") as out_f:
            out_f.write(data)

from huggingface_hub import hf_hub_download

encoder = "vits"
model_type = "Small"
model_id = f"depth_anything_v2_{encoder}"

model_path = hf_hub_download(repo_id=f"depth-anything/Depth-Anything-V2-{model_type}", filename=f"{model_id}.pth", repo_type="model")

"""# Downloading nuScenes Dataset"""

#Get image from nuscenes

!mkdir -p /data/sets/nuscenes

!wget https://www.nuscenes.org/data/v1.0-mini.tgz

!tar -xf v1.0-mini.tgz -C /data/sets/nuscenes

!pip install nuscenes-devkit &> /dev/null

"""# Displaying Sample Images"""

from nuscenes.nuscenes import NuScenes
from nuscenes.utils.data_classes import RadarPointCloud
import matplotlib.pyplot as plt
import cv2

# Load the NuScenes dataset
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)

# Choose a sample (example using the first scene and first sample)
scene = nusc.scene[0]
first_sample_token = scene['first_sample_token']
sample = nusc.get('sample', first_sample_token)

# Get the front camera data from the sample
cam_front_data = sample['data']['CAM_FRONT']

# Load the image metadata
cam_front = nusc.get('sample_data', cam_front_data)

# Load the image file
image_path = nusc.get_sample_data_path(cam_front_data)
img = cv2.imread(image_path)

#choosing other samples

# Choose a sample (example using the first scene and first sample)
scene2 = nusc.scene[3]
first_sample_token2 = scene['first_sample_token']
sample2 = nusc.get('sample', first_sample_token2)

# Get the front camera data from the sample
cam_front_data2 = sample['data']['CAM_BACK']

# Load the image metadata
cam_front2 = nusc.get('sample_data', cam_front_data2)

# Load the image file
image_path2 = nusc.get_sample_data_path(cam_front_data2)
img2 = cv2.imread(image_path2)

plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

import torch
import torch.nn.functional as F

from depth_anything_v2.dpt import DepthAnythingV2

model = DepthAnythingV2(encoder=encoder, features=64, out_channels=[48, 96, 192, 384])
model.load_state_dict(torch.load(model_path, map_location="cpu"))
model.eval()

raw_img = img
image, (h, w) = model.image2tensor(raw_img)
image = image.to("cpu").to(torch.float32)

with torch.no_grad():
    depth = model.forward(image)

depth = F.interpolate(depth[:, None], (h, w), mode="bilinear", align_corners=True)[0, 0]

output = depth.cpu().numpy()

"""# Sample Depth Estimation on nuScenes Image"""

from matplotlib import pyplot as plt
import numpy as np
import cv2


def get_depth_map(output, w, h):
    depth = cv2.resize(output, (w, h))

    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0
    depth = depth.astype(np.uint8)

    depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)

    return depth

h, w = raw_img.shape[:-1]
res_depth = get_depth_map(output, w, h)
plt.imshow(res_depth[:, :, ::-1])

"""# Downloading Detectron2 Model for 2D Object Detection"""

!python -m pip install pyyaml==5.1
import sys, os, distutils.core
# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).
# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions
!git clone 'https://github.com/facebookresearch/detectron2'
dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
sys.path.insert(0, os.path.abspath('./detectron2'))

# Properly install detectron2. (Please do not install twice in both ways)
# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

!pip install open3d

import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

"""# Running Detectron2 On Sample nuScenes Image"""

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(img)

# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
print(outputs["instances"].pred_classes)
print(outputs["instances"].pred_boxes)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

#drawing bounding box on depth map
v = Visualizer(res_depth[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

"""# Calculating Median Depths Based on Segmentation Masks"""

# print out median depths
masks = outputs["instances"].pred_masks if outputs["instances"].has("pred_masks") else None

median_depths = []
for i, mask in enumerate(masks):
    mask = mask.to("cpu").numpy()

    object_depth_values = res_depth[mask].flatten()

    object_depth_values = object_depth_values[object_depth_values > 0]

    if len(object_depth_values) > 0:
        median_depth = np.median(object_depth_values)
        median_depths.append(median_depth)
        print(f"Object {i} - Median Depth: {median_depth}")
    else:
        print(f"Object {i} has no valid depth values.")

!pip install open3d plotly

"""#Getting Camera Calibration Data from nuScenes"""

from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud
from pyquaternion import Quaternion
import numpy as np

def get_camera_intrinsics(nusc, sample_data_token):
    calibrated_sensor = nusc.get('calibrated_sensor', nusc.get('sample_data', sample_data_token)['calibrated_sensor_token'])
    return np.array(calibrated_sensor['camera_intrinsic'])

def get_camera_extrinsics(nusc, sample_data_token):
    calibrated_sensor = nusc.get('calibrated_sensor', nusc.get('sample_data', sample_data_token)['calibrated_sensor_token'])

    rotation = np.array(calibrated_sensor['rotation'])
    translation = np.array(calibrated_sensor['translation'])

    rotation_matrix = Quaternion(rotation).rotation_matrix
    extrinsic_matrix = np.eye(4)
    extrinsic_matrix[:3, :3] = rotation_matrix
    extrinsic_matrix[:3, 3] = translation
    return extrinsic_matrix

"""**Finding Center Points of Segmentation Masks**"""

from scipy.ndimage import center_of_mass

def get_center_of_segmentation_mask(masks):
  centers = []
  for mask in masks:
    mask = mask.to("cpu").numpy()
    center = center_of_mass(mask)
    centers.append(center)
  return centers

"""**Backprojection from 2D to 3D**"""

import plotly.graph_objects as go

def back_project_2d_to_3d(center_2d, depth, intrinsic_matrix):
    center_2d_homogeneous = np.array([center_2d[0], center_2d[1], 1.0])

    center_3d_camera = depth * np.linalg.inv(intrinsic_matrix).dot(center_2d_homogeneous)
    return center_3d_camera

def transform_to_world_coordinates(point_3d_camera, extrinsic_matrix):
    point_3d_camera_homogeneous = np.append(point_3d_camera, 1.0)

    point_3d_world = extrinsic_matrix.dot(point_3d_camera_homogeneous)
    return point_3d_world[:3]

import open3d as o3d

def create_point_cloud(points):
    point_cloud = o3d.geometry.PointCloud()
    point_cloud.points = o3d.utility.Vector3dVector(points)
    return point_cloud

def visualize_point_cloud(points):
    fig = go.Figure(data=[go.Scatter3d(
        x=points[:, 0],
        y=points[:, 1],
        z=points[:, 2],
        mode='markers',
        marker=dict(size=5, color='blue', opacity=0.8)
    )])
    fig.update_layout(scene=dict(
        xaxis_title='X',
        yaxis_title='Y',
        zaxis_title='Z'),
        width=800,
        height=800)
    fig.show()

"""**Visualizing Point Cloud for Sample nuScenes Image**"""

scene = nusc.scene[0]
first_sample_token = scene['first_sample_token']
first_sample = nusc.get('sample', first_sample_token)

sample_data_token = first_sample['data']['CAM_FRONT']

intrinsic = get_camera_intrinsics(nusc, sample_data_token)
extrinsic = get_camera_extrinsics(nusc, sample_data_token)

print("Intrinsic matrix (camera calibration):", intrinsic)
print("Extrinsic matrix (world to camera transformation):", extrinsic)

center_2d = get_center_of_segmentation_mask(masks)

center_3d_world_list = []

for i in range(0,len(center_2d)):
    center_3d_camera = back_project_2d_to_3d(center_2d[i], median_depths[i], intrinsic)
    center_3d_world = transform_to_world_coordinates(center_3d_camera, extrinsic)
    center_3d_world_list.append(center_3d_world)

points_3d = np.array(center_3d_world_list)

visualize_point_cloud(points_3d)

"""**Converting 3D Point Cloud to Bird's Eye View**"""

import numpy as np
import matplotlib.pyplot as plt

def convert_to_bev(points, grid_size, range_x, range_y):
    # Create an empty BEV grid
    bev_grid = np.zeros((grid_size[1], grid_size[0]))

    # Convert points to grid coordinates
    for point in points:
        x, y, z = point

        # Define grid resolution
        grid_x = int((x - range_x[0]) / (range_x[1] - range_x[0]) * grid_size[0])
        grid_y = int((y - range_y[0]) / (range_y[1] - range_y[0]) * grid_size[1])

        # Check bounds
        if 0 <= grid_x < grid_size[0] and 0 <= grid_y < grid_size[1]:
            # Accumulate height information or count points in the cell
            bev_grid[grid_y, grid_x] += 1  # Here we're counting the points

    return bev_grid

def visualize_bev(bev_grid):
    plt.imshow(bev_grid, cmap='hot', interpolation='nearest')
    plt.colorbar(label='Point Count')
    plt.xlabel('X Grid Index')
    plt.ylabel('Y Grid Index')
    plt.title('Bird\'s Eye View')
    plt.show()

grid_size = (50, 50)  # Size of the BEV grid (width, height)
range_x = (0, 50)        # X range of the grid
range_y = (0, 50)        # Y range of the grid

# Convert to BEV
bev_grid = convert_to_bev(points_3d, grid_size, range_x, range_y)

# Visualize BEV
visualize_bev(bev_grid)

"""#HEURISTICS
get width, height, length average for cars and trucks (no pedestrians etc) then plot a 3d cuboid around each central point from the graph that we plotted already

This is for dataset 0
"""



"""This is for randomized datasets. Not all of them show up in the 3D cuboid


"""

import random
from nuscenes.nuscenes import NuScenes
from nuscenes.utils.data_classes import Box
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import matplotlib.pyplot as plt
import numpy as np

# Load the NuScenes mini dataset
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)

# Function to plot a 3D cuboid based on the bounding box
def plot_3d_cuboid(ax, box: Box):
    corners = box.corners().T  # Get corners of the bounding box
    faces = [
        [corners[j] for j in [0, 1, 2, 3]],  # Bottom face
        [corners[j] for j in [4, 5, 6, 7]],  # Top face
        [corners[j] for j in [0, 3, 7, 4]],  # Left face
        [corners[j] for j in [1, 2, 6, 5]],  # Right face
        [corners[j] for j in [0, 1, 5, 4]],  # Front face
        [corners[j] for j in [3, 2, 6, 7]]   # Back face
    ]
    ax.add_collection3d(Poly3DCollection(faces, color='blue', alpha=0.3, edgecolor='k'))

# Function to get a valid sample with vehicle annotations
def get_valid_sample():
    while True:
        # Randomly select a scene
        random_scene = random.choice(nusc.scene)
        first_sample_token = random_scene['first_sample_token']

        # Collect all sample tokens in this scene
        sample_token = first_sample_token
        sample_tokens = []
        while sample_token:
            sample_tokens.append(sample_token)
            sample = nusc.get('sample', sample_token)
            sample_token = sample['next']

        # Randomly select a sample from the scene
        random_sample_token = random.choice(sample_tokens)
        sample = nusc.get('sample', random_sample_token)

        # Check if this sample has any relevant vehicle types
        has_vehicle = any(
            'vehicle.car' in ann['category_name'] or
            'vehicle.truck' in ann['category_name'] or
            'vehicle.bus.rigid' in ann['category_name']
            for ann_token in sample['anns']
            for ann in [nusc.get('sample_annotation', ann_token)]
        )

        if has_vehicle:
            print(f"Selected scene: {random_scene['name']}")
            print(f"Selected sample token: {random_sample_token}")
            return sample

# Get a valid sample with vehicles
sample = get_valid_sample()

# Initialize min and max values for setting dynamic plot limits
min_x, min_y, min_z = float('inf'), float('inf'), float('inf')
max_x, max_y, max_z = float('-inf'), float('-inf'), float('-inf')

# Find min and max coordinates for the sample
for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    category_name = ann['category_name']

    if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
        box = nusc.get_box(ann['token'])

        if box is not None:
            corners = box.corners()
            min_x = min(min_x, corners[0].min())
            max_x = max(max_x, corners[0].max())
            min_y = min(min_y, corners[1].min())
            max_y = max(max_y, corners[1].max())
            min_z = min(min_z, corners[2].min())
            max_z = max(max_z, corners[2].max())

# Add some padding to the limits for better visibility
padding = 5
min_x, max_x = min_x - padding, max_x + padding
min_y, max_y = min_y - padding, max_y + padding
min_z, max_z = min_z - padding, max_z + padding

# Set up the 3D plot with dynamic limits
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_xlim(min_x, max_x)
ax.set_ylim(min_y, max_y)
ax.set_zlim(min_z, max_z)

# Plot each vehicle cuboid
for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    category_name = ann['category_name']

    if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
        box = nusc.get_box(ann['token'])

        if box is not None:
            print(f"Plotting box for {category_name} at {box.center}")
            plot_3d_cuboid(ax, box)

# Set labels and show plot
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
ax.set_zlabel('Z-axis')
ax.set_title('3D Cuboids for Detected Vehicles')
plt.show()

import random
import math
from nuscenes.nuscenes import NuScenes
from nuscenes.utils.data_classes import Box
import matplotlib.pyplot as plt
import numpy as np

# Load the NuScenes mini dataset
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)

# Function to plot a 3D cuboid based on the bounding box
def plot_3d_cuboid(ax, box: Box):
    corners = box.corners().T  # Get corners of the bounding box
    faces = [
        [corners[j] for j in [0, 1, 2, 3]],  # Bottom face
        [corners[j] for j in [4, 5, 6, 7]],  # Top face
        [corners[j] for j in [0, 3, 7, 4]],  # Left face
        [corners[j] for j in [1, 2, 6, 5]],  # Right face
        [corners[j] for j in [0, 1, 5, 4]],  # Front face
        [corners[j] for j in [3, 2, 6, 7]]   # Back face
    ]
    ax.add_collection3d(Poly3DCollection(faces, color='blue', alpha=0.3, edgecolor='k'))

# Function to rotate points by a given angle in radians
def rotate_points(points, angle_rad):
    """Rotate points by a given angle in radians."""
    rotation_matrix = np.array([
        [math.cos(angle_rad), -math.sin(angle_rad)],
        [math.sin(angle_rad), math.cos(angle_rad)]
    ])
    return np.dot(points, rotation_matrix.T)

# Function to plot a 2D bounding box for bird's-eye view
def plot_2d_bounding_box(ax, rotated_corners):
    x_coords = np.append(rotated_corners[:, 0], rotated_corners[0, 0])  # Close the loop
    y_coords = np.append(rotated_corners[:, 1], rotated_corners[0, 1])
    ax.plot(x_coords, y_coords, color='blue', alpha=0.5, linewidth=2)

# Function to get a valid sample with vehicle annotations
def get_valid_sample():
    while True:
        # Randomly select a scene
        random_scene = random.choice(nusc.scene)
        first_sample_token = random_scene['first_sample_token']

        # Collect all sample tokens in this scene
        sample_token = first_sample_token
        sample_tokens = []
        while sample_token:
            sample_tokens.append(sample_token)
            sample = nusc.get('sample', sample_token)
            sample_token = sample['next']

        # Randomly select a sample from the scene
        random_sample_token = random.choice(sample_tokens)
        sample = nusc.get('sample', random_sample_token)

        # Check if this sample has any relevant vehicle types
        has_vehicle = any(
            'vehicle.car' in ann['category_name'] or
            'vehicle.truck' in ann['category_name'] or
            'vehicle.bus.rigid' in ann['category_name']
            for ann_token in sample['anns']
            for ann in [nusc.get('sample_annotation', ann_token)]
        )

        if has_vehicle:
            print(f"Selected scene: {random_scene['name']}")
            print(f"Selected sample token: {random_sample_token}")
            return sample

# Get a valid sample with vehicles
sample = get_valid_sample()

# Get the ego vehicle's yaw angle from the LiDAR data's ego pose
lidar_data = nusc.get('sample_data', sample['data']['LIDAR_TOP'])
ego_pose = nusc.get('ego_pose', lidar_data['ego_pose_token'])
yaw_angle = math.atan2(ego_pose['rotation'][1], ego_pose['rotation'][0])  # Get yaw in radians

# Initialize min and max values for setting dynamic plot limits
min_x, min_y, min_z = float('inf'), float('inf'), float('inf')
max_x, max_y, max_z = float('-inf'), float('-inf'), float('-inf')

# Find min and max coordinates for the sample
for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    category_name = ann['category_name']

    if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
        box = nusc.get_box(ann['token'])

        if box is not None:
            corners = box.corners()
            min_x = min(min_x, corners[0].min())
            max_x = max(max_x, corners[0].max())
            min_y = min(min_y, corners[1].min())
            max_y = max(max_y, corners[1].max())
            min_z = min(min_z, corners[2].min())
            max_z = max(max_z, corners[2].max())

# Add padding to the limits for better visibility
padding = 5
min_x, max_x = min_x - padding, max_x + padding
min_y, max_y = min_y - padding, max_y + padding
min_z, max_z = min_z - padding, max_z + padding

# Set up the figure with two subplots for 3D cuboid and 2D bird's-eye view
fig = plt.figure(figsize=(18, 6))

# 1. 3D Cuboid Plot
ax1 = fig.add_subplot(121, projection='3d')
ax1.set_xlim(min_x, max_x)
ax1.set_ylim(min_y, max_y)
ax1.set_zlim(min_z, max_z)

# Plot each vehicle cuboid in 3D
for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    category_name = ann['category_name']

    if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
        box = nusc.get_box(ann['token'])

        if box is not None:
            print(f"Plotting 3D box for {category_name} at {box.center}")
            plot_3d_cuboid(ax1, box)

ax1.set_xlabel('X-axis')
ax1.set_ylabel('Y-axis')
ax1.set_zlabel('Z-axis')
ax1.set_title('3D Cuboids of Detected Vehicles')

# 2. Adjusted Bird's-Eye View (Top-Down) Plot in 2D
ax2 = fig.add_subplot(122)
ax2.set_xlim(min_x, max_x)
ax2.set_ylim(min_y, max_y)

# Rotate each bounding box by the ego vehicle's yaw angle and plot
for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    category_name = ann['category_name']

    if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
        box = nusc.get_box(ann['token'])

        if box is not None:
            # Extract x, y corners and apply rotation
            corners = box.corners().T[:, :2]  # Take only x, y points
            rotated_corners = rotate_points(corners, yaw_angle)
            plot_2d_bounding_box(ax2, rotated_corners)

ax2.set_xlabel('X-axis')
ax2.set_ylabel('Y-axis')
ax2.set_title('Bird\'s-Eye View of Detected Vehicles')

# Display the two plots
plt.tight_layout()
plt.show()

# 3. LiDAR Plot using `nusc.render_sample_data`
# Render the LiDAR data with multiple sweeps and an underlay map
nusc.render_sample_data(sample['data']['LIDAR_TOP'], nsweeps=5, underlay_map=True)

import math
from nuscenes.nuscenes import NuScenes
from nuscenes.utils.data_classes import Box
import matplotlib.pyplot as plt
import numpy as np

# Load the NuScenes mini dataset
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)

# Function to plot a 3D cuboid based on the bounding box
def plot_3d_cuboid(ax, box: Box):
    corners = box.corners().T  # Get corners of the bounding box
    faces = [
        [corners[j] for j in [0, 1, 2, 3]],  # Bottom face
        [corners[j] for j in [4, 5, 6, 7]],  # Top face
        [corners[j] for j in [0, 3, 7, 4]],  # Left face
        [corners[j] for j in [1, 2, 6, 5]],  # Right face
        [corners[j] for j in [0, 1, 5, 4]],  # Front face
        [corners[j] for j in [3, 2, 6, 7]]   # Back face
    ]
    ax.add_collection3d(Poly3DCollection(faces, color='blue', alpha=0.3, edgecolor='k'))

# Function to rotate points by a given angle in radians
def rotate_points(points, angle_rad):
    """Rotate points by a given angle in radians."""
    rotation_matrix = np.array([
        [math.cos(angle_rad), -math.sin(angle_rad)],
        [math.sin(angle_rad), math.cos(angle_rad)]
    ])
    return np.dot(points, rotation_matrix.T)

# Function to plot a 2D bounding box for bird's-eye view
def plot_2d_bounding_box(ax, rotated_corners):
    x_coords = np.append(rotated_corners[:, 0], rotated_corners[0, 0])  # Close the loop
    y_coords = np.append(rotated_corners[:, 1], rotated_corners[0, 1])
    ax.plot(x_coords, y_coords, color='blue', alpha=0.5, linewidth=2)

# Function to get a specific sample based on scene name and sample index
def get_sample_by_index(scene_name, sample_index):
    # Find the scene by name
    selected_scene = next((scene for scene in nusc.scene if scene['name'] == scene_name), None)
    if not selected_scene:
        print(f"Scene '{scene_name}' not found.")
        return None

    # Collect all sample tokens in this scene
    sample_token = selected_scene['first_sample_token']
    sample_tokens = []
    while sample_token:
        sample_tokens.append(sample_token)
        sample = nusc.get('sample', sample_token)
        sample_token = sample['next']

    # Check if the specified index is valid
    if sample_index >= len(sample_tokens):
        print(f"Sample index {sample_index} out of range for scene '{scene_name}'")
        return None

    # Retrieve the sample at the specified index
    sample_token = sample_tokens[sample_index]
    sample = nusc.get('sample', sample_token)

    print(f"Selected scene: {scene_name}")
    print(f"Selected sample index: {sample_index} with sample token: {sample_token}")
    return sample

# Choose a specific scene and sample index
scene_name = 'scene-0061'  # Replace with desired scene name
sample_index = 5  # Replace with desired sample index

# Get the selected sample
sample = get_sample_by_index(scene_name, sample_index)
if sample is None:
    print("Invalid scene or sample index.")
else:
    # Get the ego vehicle's yaw angle from the LiDAR data's ego pose
    lidar_data = nusc.get('sample_data', sample['data']['LIDAR_TOP'])
    ego_pose = nusc.get('ego_pose', lidar_data['ego_pose_token'])
    yaw_angle = math.atan2(ego_pose['rotation'][1], ego_pose['rotation'][0])  # Get yaw in radians

    # Initialize min and max values for setting dynamic plot limits
    min_x, min_y, min_z = float('inf'), float('inf'), float('inf')
    max_x, max_y, max_z = float('-inf'), float('-inf'), float('-inf')

    # Find min and max coordinates for the sample
    for ann_token in sample['anns']:
        ann = nusc.get('sample_annotation', ann_token)
        category_name = ann['category_name']

        if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
            box = nusc.get_box(ann['token'])

            if box is not None:
                corners = box.corners()
                min_x = min(min_x, corners[0].min())
                max_x = max(max_x, corners[0].max())
                min_y = min(min_y, corners[1].min())
                max_y = max(max_y, corners[1].max())
                min_z = min(min_z, corners[2].min())
                max_z = max(max_z, corners[2].max())

    # Add padding to the limits for better visibility
    padding = 5
    min_x, max_x = min_x - padding, max_x + padding
    min_y, max_y = min_y - padding, max_y + padding
    min_z, max_z = min_z - padding, max_z + padding

    # Set up the figure with two subplots for 3D cuboid and 2D bird's-eye view
    fig = plt.figure(figsize=(18, 6))

    # 1. 3D Cuboid Plot
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.set_xlim(min_x, max_x)
    ax1.set_ylim(min_y, max_y)
    ax1.set_zlim(min_z, max_z)

    # Plot each vehicle cuboid in 3D
    for ann_token in sample['anns']:
        ann = nusc.get('sample_annotation', ann_token)
        category_name = ann['category_name']

        if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
            box = nusc.get_box(ann['token'])

            if box is not None:
                print(f"Plotting 3D box for {category_name} at {box.center}")
                plot_3d_cuboid(ax1, box)

    ax1.set_xlabel('X-axis')
    ax1.set_ylabel('Y-axis')
    ax1.set_zlabel('Z-axis')
    ax1.set_title('3D Cuboids of Detected Vehicles')

    # 2. Adjusted Bird's-Eye View (Top-Down) Plot in 2D
    ax2 = fig.add_subplot(122)
    ax2.set_xlim(min_x, max_x)
    ax2.set_ylim(min_y, max_y)

    # Rotate each bounding box by the ego vehicle's yaw angle and plot
    for ann_token in sample['anns']:
        ann = nusc.get('sample_annotation', ann_token)
        category_name = ann['category_name']

        if category_name in ['vehicle.car', 'vehicle.truck', 'vehicle.bus.rigid']:
            box = nusc.get_box(ann['token'])

            if box is not None:
                # Extract x, y corners and apply rotation
                corners = box.corners().T[:, :2]  # Take only x, y points
                rotated_corners = rotate_points(corners, yaw_angle)
                plot_2d_bounding_box(ax2, rotated_corners)

    ax2.set_xlabel('X-axis')
    ax2.set_ylabel('Y-axis')
    ax2.set_title('Bird\'s-Eye View of Detected Vehicles')

    # Display the two plots
    plt.tight_layout()
    plt.show()

    # 3. LiDAR Plot using `nusc.render_sample_data`
    # Render the LiDAR data with multiple sweeps and an underlay map
    nusc.render_sample_data(sample['data']['LIDAR_TOP'], nsweeps=5, underlay_map=True)

COCO_CLASSES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
    "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
    "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella",
    "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat",
    "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog",
    "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "dining table", "toilet", "TV",
    "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink",
    "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]

pred_classes = outputs["instances"].pred_classes
pred_class_names = [COCO_CLASSES[i] for i in pred_classes]
for i in pred_class_names:
  print(i)

from mpl_toolkits.mplot3d.art3d import Poly3DCollection
# Average dimensions for cars and trucks in meters (Width, Height, Length)
average_dimensions = {
    'car': (1.8, 1.5, 4.5),
    'truck': (2.5, 3.0, 8.0)
}

cuboids = []

for i, category in enumerate(pred_class_names):
    if category in average_dimensions:
        width, height, length = average_dimensions[category]
        center_point = center_3d_world_list[i]

        cuboids.append((center_point, width, height, length))

def plot_multiple_cuboids(cuboids):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection="3d")

    for cuboid in cuboids:
        center, width, height, length = cuboid
        x_c, y_c, z_c = center
        dx = width / 2
        dy = length / 2
        dz = height / 2

        vertices = np.array([
            [x_c - dx, y_c - dy, z_c - dz],
            [x_c + dx, y_c - dy, z_c - dz],
            [x_c + dx, y_c + dy, z_c - dz],
            [x_c - dx, y_c + dy, z_c - dz],
            [x_c - dx, y_c - dy, z_c + dz],
            [x_c + dx, y_c - dy, z_c + dz],
            [x_c + dx, y_c + dy, z_c + dz],
            [x_c - dx, y_c + dy, z_c + dz]
        ])

        faces = [
            [vertices[0], vertices[1], vertices[2], vertices[3]],  # Bottom face
            [vertices[4], vertices[5], vertices[6], vertices[7]],  # Top face
            [vertices[0], vertices[1], vertices[5], vertices[4]],  # Front face
            [vertices[2], vertices[3], vertices[7], vertices[6]],  # Back face
            [vertices[1], vertices[2], vertices[6], vertices[5]],  # Right face
            [vertices[0], vertices[3], vertices[7], vertices[4]]   # Left face
        ]

        ax.add_collection3d(Poly3DCollection(faces, facecolors="cyan", linewidths=1, edgecolors="r", alpha=.25))

    all_x = [center[0] for center, _, _, _ in cuboids]
    all_y = [center[1] for center, _, _, _ in cuboids]
    all_z = [center[2] for center, _, _, _ in cuboids]
    max_dim = max([width for _, width, _, _ in cuboids] + [length for _, _, length, _ in cuboids] + [height for _, _, _, height in cuboids])

    ax.set_xlim(min(all_x) - max_dim, max(all_x) + max_dim)
    ax.set_ylim(min(all_y) - max_dim, max(all_y) + max_dim)
    ax.set_zlim(min(all_z) - max_dim, max(all_z) + max_dim)

    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')

    plt.show()

plot_multiple_cuboids(cuboids)

from matplotlib.patches import Polygon
from matplotlib.collections import PatchCollection

def plot_multiple_cuboids_2d(cuboids):
    fig, ax = plt.subplots()

    patches = []

    for cuboid in cuboids:
        center, width, length, height = cuboid
        x_c, y_c, _ = center
        dx = width / 2
        dy = length / 2

        vertices = np.array([
            [x_c - dx, y_c - dy],
            [x_c + dx, y_c - dy],
            [x_c + dx, y_c + dy],
            [x_c - dx, y_c + dy]
        ])

        polygon = Polygon(vertices, closed=True)
        patches.append(polygon)

    p = PatchCollection(patches, facecolor="cyan", edgecolor="r", alpha=0.5)
    ax.add_collection(p)

    all_x = [center[0] for center, _, _, _ in cuboids]
    all_y = [center[1] for center, _, _, _ in cuboids]
    max_dim = max([width for _, width, _, _ in cuboids] + [length for _, _, length, _ in cuboids])

    ax.set_xlim(min(all_x) - max_dim, max(all_x) + max_dim)
    ax.set_ylim(min(all_y) - max_dim, max(all_y) + max_dim)

    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_aspect('equal', 'box')

    plt.show()

plot_multiple_cuboids_2d(cuboids)
#plot w. width and length
#add box/dot that represents autonomous vehicle
#plot nuscenes ground truth cuboids and compare with generated 3d cuboids
#nuscenes stops at 30m

#orientation: use map prior lane markings/lines

scene = nusc.scene[0]
first_sample_token = scene['first_sample_token']
sample = nusc.get('sample', first_sample_token)
nusc.render_sample_data(sample['data']['LIDAR_TOP'], nsweeps=5, underlay_map=True)
#challenging scene due to orientation of av
#pick example w. straight orientation
#(INSTEAD OF 3D CUBOIDS) plot BEV prediction boxes onto the ground truth
#see if we can get uncropped lidar plot
#query for closest lane line to center point, apply that to orient